{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import threadpool\n",
    "\n",
    "sys.path.insert(0, sys.path[0] + '/..')\n",
    "# sys.path.append(\"..\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    def __init__(self):\n",
    "        self.data='/iesl/canvas/hanqingli/NSD_for_sentence_embedding/data/wiki2016_min100/'\n",
    "        self.save='/iesl/canvas/hanqingli/NSD_for_sentence_embedding/data/wiki2016_min100/tensors'\n",
    "        self.max_sent_len=50\n",
    "        self.multi_sent=False\n",
    "        self.max_target_num=30\n",
    "        self.max_sent_num=1000\n",
    "        self.target_num_prev = 5\n",
    "        self.target_num_next=5\n",
    "        self.seed=1038\n",
    "        self.stop_word_file='/iesl/canvas/hanqingli/NSD_for_sentence_embedding/data/stop_word_list'\n",
    "args=arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)\n",
    "\n",
    "random.seed(args.seed)\n",
    "\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "\n",
    "summarize = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stop_to_ind(f_in, w_d2_ind_freq):\n",
    "    stop_word_set = set()\n",
    "    for line in f_in:\n",
    "        w = line.rstrip()\n",
    "        if w in w_d2_ind_freq:\n",
    "            stop_word_set.add(w_d2_ind_freq[w][0])\n",
    "    return stop_word_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stop_to_ind_lower(f_in, idx2word_freq):\n",
    "    stop_word_org_set = set()\n",
    "    for line in f_in:\n",
    "        w = line.rstrip()\n",
    "        stop_word_org_set.add(w)\n",
    "    stop_word_set = set()\n",
    "    for idx, (w, freq) in enumerate(idx2word_freq):\n",
    "        if w.lower() in stop_word_org_set:\n",
    "            stop_word_set.add(idx)\n",
    "    return stop_word_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w_ind(f_in, max_sent_num, max_sent_len):\n",
    "    w_ind_corpus = []\n",
    "    last_sent = ''\n",
    "    num_duplicated_sent = 0\n",
    "    num_too_long_sent = 0\n",
    "\n",
    "    for line in f_in:\n",
    "        current_sent = line.rstrip()\n",
    "        if current_sent == last_sent:\n",
    "            num_duplicated_sent += 1\n",
    "            continue\n",
    "        last_sent = current_sent\n",
    "        fields = current_sent.split(' ')\n",
    "        if len(fields) > max_sent_len:\n",
    "            num_too_long_sent += 1\n",
    "            continue\n",
    "        w_ind_corpus.append([int(x) for x in fields])\n",
    "        if len(w_ind_corpus) % 1000000 == 0:\n",
    "            print(len(w_ind_corpus))\n",
    "            sys.stdout.flush()\n",
    "        if len(w_ind_corpus) > max_sent_num:\n",
    "            break\n",
    "    print(\"Finish loading {} sentences. While removing {} duplicated and {} long sentences\".format(len(w_ind_corpus),\n",
    "                                                                                                   num_duplicated_sent,\n",
    "                                                                                                   num_too_long_sent))\n",
    "    return w_ind_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_input_name = args.data + \"corpus_index\"\n",
    "dictionary_input_name = args.data + \"dictionary_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dictionary_input_name) as f_in:\n",
    "    idx2word_freq = utils.load_idx2word_freq(f_in)\n",
    "\n",
    "max_ind = len(idx2word_freq)\n",
    "\n",
    "if max_ind >= 2147483648:\n",
    "    print(\"Will cause overflow\")\n",
    "    sys.exit()\n",
    "\n",
    "store_type = torch.int32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.stop_word_file) as f_in:\n",
    "    # stop_ind_set = convert_stop_to_ind(f_in, w_d2_ind_freq)\n",
    "    stop_ind_set = convert_stop_to_ind_lower(f_in, idx2word_freq)\n",
    "\n",
    "with open(corpus_input_name) as f_in:\n",
    "    w_ind_corpus = load_w_ind(f_in, args.max_sent_num, args.max_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(w_ind_corpus) - 2\n",
    "# args.max_target_num+args.max_sent_len\n",
    "print(\"Allocating {} bytes\".format(corpus_size * (args.max_target_num + args.max_sent_len) * 4))\n",
    "all_features = torch.zeros(corpus_size, args.max_sent_len, dtype=store_type)\n",
    "all_targets = torch.zeros(corpus_size, args.max_sent_len, dtype=store_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection_num = 0\n",
    "\n",
    "\n",
    "# for i in range(1,len(w_ind_corpus)-1):\n",
    "    \n",
    "def match(i):\n",
    "    output_i = i - 1\n",
    "    if args.multi_sent:\n",
    "        current_len = 0\n",
    "        feature_list = []\n",
    "        for j in range(i,len(w_ind_corpus)-1):\n",
    "            w_ind_list = w_ind_corpus[j][:-1] #excluding <eos>\n",
    "            sent_len = len(w_ind_list)\n",
    "            current_len_prev = current_len\n",
    "            current_len += sent_len\n",
    "            if current_len > args.max_sent_len - 1:\n",
    "                break\n",
    "            feature_list += w_ind_list\n",
    "        if current_len > args.max_sent_len - 1:\n",
    "            current_len = current_len_prev\n",
    "        feature_list.append(w_ind_corpus[j-1][-1])\n",
    "        next_sent_ind = j\n",
    "    else:\n",
    "        feature_list = w_ind_corpus[i]\n",
    "#         current_len = len(feature_list) - 1\n",
    "        next_sent_ind = i + 1\n",
    "#     print(len(feature_list))\n",
    "    all_features[output_i,-len(feature_list):] = torch.tensor(feature_list,dtype = store_type)\n",
    "    prev_w_ind_list = w_ind_corpus[i-args.target_num_prev:i-1]\n",
    "    next_w_ind_list = w_ind_corpus[next_sent_ind:next_sent_ind+args.target_num_next-1]\n",
    "    target_list = prev_w_ind_list+next_w_ind_list\n",
    "    target_list = sum(target_list,[])\n",
    "\n",
    "    target_w_list = []\n",
    "    for w in target_list:\n",
    "        if w not in stop_ind_set:\n",
    "            target_w_list.append(w)\n",
    " \n",
    "    # Count the num of appearance of feature words\n",
    "    word_count = []\n",
    "    from collections import Counter\n",
    "    counter = Counter(target_w_list)\n",
    "    for cnt, word in enumerate(feature_list):\n",
    "        number = counter[word]\n",
    "        word_count.append(number)\n",
    "\n",
    "    all_targets[output_i, -len(word_count):] = torch.tensor(word_count, dtype = store_type)\n",
    "    for row in all_targets:\n",
    "        num = int(torch.sum(row))\n",
    "#         print(num)\n",
    "        if num not in summarize:\n",
    "            summarize[num]=1\n",
    "        else:\n",
    "            summarize[num] += 1\n",
    "\n",
    "pool=threadpool.ThreadPool(32)\n",
    "requests = threadpool.makeRequests(match, range(1,len(w_ind_corpus)-1))\n",
    "[pool.putRequest(req) for req in requests]\n",
    "pool.wait()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del w_ind_corpus\n",
    "training_output_name = os.path.join(args.save, \"train.pt\")\n",
    "val_org_output_name = os.path.join(args.save, \"val_org.pt\")\n",
    "test_org_output_name = os.path.join(args.save, \"test_org.pt\")\n",
    "val_shuffled_output_name = os.path.join(args.save, \"val_shuffled.pt\")\n",
    "test_shuffled_output_name = os.path.join(args.save, \"test_shuffled.pt\")\n",
    "\n",
    "testing_size_ratio = 0.05\n",
    "testing_size = int(corpus_size * testing_size_ratio)\n",
    "print(\"Testing size: {}\".format(testing_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_tensors(f_out, tensor1, tensor2):\n",
    "    torch.save([tensor1, tensor2], f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(test_org_output_name, 'wb') as f_out:\n",
    "#     store_tensors(f_out, all_features[-testing_size:, :].clone(), all_targets[-testing_size:, :].clone())\n",
    "\n",
    "# with open(val_org_output_name, 'wb') as f_out:\n",
    "#     store_tensors(f_out, all_features[-2 * testing_size:-testing_size, :].clone(),\n",
    "#                   all_targets[-2 * testing_size:-testing_size, :].clone())\n",
    "\n",
    "# rest_size = corpus_size - 2 * testing_size\n",
    "# shuffle_ind = list(range(rest_size))\n",
    "# random.shuffle(shuffle_ind)\n",
    "\n",
    "# with open(test_shuffled_output_name, 'wb') as f_out:\n",
    "#     store_ind = shuffle_ind[-testing_size:]\n",
    "#     store_tensors(f_out, all_features[store_ind, :], all_targets[store_ind, :])\n",
    "\n",
    "# with open(val_shuffled_output_name, 'wb') as f_out:\n",
    "#     store_ind = shuffle_ind[-2 * testing_size:-testing_size]\n",
    "#     store_tensors(f_out, all_features[store_ind, :], all_targets[store_ind, :])\n",
    "\n",
    "# with open(training_output_name, 'wb') as f_out:\n",
    "#     store_ind = shuffle_ind[:rest_size - 2 * testing_size]\n",
    "#     print(\"Training size: {}\".format(len(store_ind)))\n",
    "#     store_tensors(f_out, all_features[store_ind, :], all_targets[store_ind, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
