{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "#import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cutorch\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "import random\n",
    "import model as model_code\n",
    "import nsd_loss\n",
    "\n",
    "from utils import seed_all_randomness, create_exp_dir, save_checkpoint, load_idx2word_freq, load_emb_file_to_tensor, load_corpus, output_parallel_models, str2bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    def __init__(self):\n",
    "        # path\n",
    "        self.data = '/iesl/canvas/hanqingli/NSD_for_sentence_embedding/data/wiki2016_min100/5_5/'\n",
    "        self.tensor_folder='corpus_00'\n",
    "        self.training_file='train.pt'\n",
    "        self.save='/iesl/canvas/hanqingli/NSD_for_sentence_embedding/saves/test1'\n",
    "        self.emb_file='/iesl/canvas/hschang/language_modeling/NSD_for_sentence_embedding/resources/glove.840B.300d_filtered_wiki2016.txt'\n",
    "        # parser.add_argument('--stop_word_file', type=str, default='./resources/stop_word_list',\n",
    "        #                    help='path to the file of a stop word list')\n",
    "\n",
    "        # encoder\n",
    "        # both\n",
    "#         parser.add_argument('--en_model', type=str, default='LSTM',\n",
    "#                             help='type of encoder model (LSTM, LSTM+TRANS, TRANS+LSTM, TRANS)')\n",
    "        self.en_model='TRANS'\n",
    "#         parser.add_argument('--emsize', type=int, default=0,\n",
    "#                             help='size of word embeddings')\n",
    "        self.emsize=0\n",
    "#         parser.add_argument('--dropouti', type=float, default=0.4,\n",
    "#                             help='dropout for input embedding layers (0 = no dropout)')\n",
    "        self.dropouti=0.1\n",
    "#         parser.add_argument('--dropoute', type=float, default=0.1,\n",
    "#                             help='dropout to remove words from embedding layer (0 = no dropout)')\n",
    "        self.dropoute=0\n",
    "        # LSTM only\n",
    "#         parser.add_argument('--nhid', type=int, default=600,\n",
    "#                             help='number of hidden units per layer in LSTM')\n",
    "        self.nhid=600\n",
    "#         parser.add_argument('--nlayers', type=int, default=1,\n",
    "#                             help='number of layers')\n",
    "        self.nlayers=1\n",
    "#         parser.add_argument('--dropout', type=float, default=0.4,\n",
    "#                             help='dropout applied to the output layer (0 = no dropout)')\n",
    "        self.dropout=0\n",
    "        # TRANS only\n",
    "#         parser.add_argument('--encode_trans_layers', type=int, default=5,\n",
    "#                             help='How many layers we have in transformer. Do not have effect if de_model is LSTM')\n",
    "        self.encode_trans_layers=5\n",
    "#         parser.add_argument('--trans_nhid', type=int, default=-1,\n",
    "#                             help='number of hidden units per layer in transformer')\n",
    "        self.trans_nhid=-1\n",
    "#         parser.add_argument('--nhidlast', type=int, default=-1,\n",
    "#                             help='number of hidden units for the last rnn layer')\n",
    "        self.nhidlast=-1\n",
    "#         parser.add_argument('--dropouth', type=float, default=0.3,\n",
    "#                             help='dropout for rnn layers (0 = no dropout)')\n",
    "        self.dropouth=0.3\n",
    "#         parser.add_argument('--dropoutl', type=float, default=-0.2,\n",
    "#                             help='dropout applied to layers (0 = no dropout)')\n",
    "        self.dropoutl=-0.25\n",
    "#         parser.add_argument('--wdrop', type=float, default=0.5,\n",
    "#                             help='amount of weight dropout to apply to the RNN hidden to hidden matrix')\n",
    "        self.wdrop=0.5\n",
    "\n",
    "        # decoder\n",
    "        # both\n",
    "#         parser.add_argument('--de_model', type=str, default='LSTM',\n",
    "#                             help='type of decoder model (LSTM, LSTM+TRANS, TRANS+LSTM, TRANS)')\n",
    "#         parser.add_argument('--de_coeff_model', type=str, default='LSTM',\n",
    "#                             help='type of decoder model to predict coefficients (LSTM, TRANS)')\n",
    "#         parser.add_argument('--n_basis', type=int, default=10,\n",
    "#                             help='number of basis we want to predict')\n",
    "#         # parser.add_argument('--linear_mapping_dim', type=int, default=0,\n",
    "#         #                    help='map the input embedding by linear transformation')\n",
    "#         parser.add_argument('--positional_option', type=str, default='linear',\n",
    "#                             help='options of encode positional embedding into models (linear, cat, add)')\n",
    "#         parser.add_argument('--dropoutp', type=float, default=0.5,\n",
    "#                             help='dropout of positional embedding or input embedding after linear transformation (when linear_mapping_dim != 0)')\n",
    "#         # LSTM only\n",
    "#         parser.add_argument('--nhidlast2', type=int, default=-1,\n",
    "#                             help='hidden embedding size of the second LSTM')\n",
    "#         # TRANS only\n",
    "#         parser.add_argument('--trans_layers', type=int, default=5,\n",
    "#                             help='How many layers we have in transformer. Do not have effect if de_model is LSTM')\n",
    "#         parser.add_argument('--de_en_connection', type=str2bool, nargs='?', default=True,\n",
    "#                             help='If True, using Transformer decoder in our decoder. Otherwise, using Transformer encoder')\n",
    "#         parser.add_argument('--dropout_prob_trans', type=float, default=0.1,\n",
    "#                             help='hidden_dropout_prob and attention_probs_dropout_prob in Transformer')\n",
    "        # coeff\n",
    "#         parser.add_argument('--w_loss_coeff', type=float, default=0.1,\n",
    "#                             help='weights for coefficient prediction loss')\n",
    "        self.w_loss_coeff=0.1\n",
    "#         parser.add_argument('--L1_losss_B', type=float, default=0.2,\n",
    "#                             help='L1 loss for the coefficient matrix')\n",
    "        self.L1_losss_B=0.25\n",
    "        # parser.add_argument('--coeff_opt', type=str, default='max',\n",
    "#         parser.add_argument('--coeff_opt', type=str, default='lc',\n",
    "#                             help='Could be max, lc, maxlc')\n",
    "        self.coeff_opt='lc'\n",
    "#         parser.add_argument('--coeff_opt_algo', type=str, default='rmsprop',\n",
    "#                             # parser.add_argument('--coeff_opt_algo', type=str, default='sgd_bmm',\n",
    "#                             help='Could be sgd_bmm, sgd, asgd, adagrad, rmsprop, and adam')\n",
    "        self.coeff_opt_algo='rmsprop'\n",
    "        # target emb\n",
    "#         parser.add_argument('--update_target_emb', default=False, action='store_true',\n",
    "#                             help='Whether to update target embedding')\n",
    "        self.update_target_emb=False\n",
    "#         parser.add_argument('--target_emb_source', type=str, default='ext',\n",
    "#                             help='Could be ext (external), rand or ewe (encode word embedding)')\n",
    "        self.target_emb_source='ext'\n",
    "\n",
    "        # training\n",
    "#         parser.add_argument('--optimizer', type=str, default=\"SGD\",\n",
    "#                             help='optimization algorithm. Could be SGD or Adam')\n",
    "        self.optimizer='SGD'\n",
    "#         parser.add_argument('--lr', type=float, default=1,\n",
    "#                             help='initial learning rate')\n",
    "        self.lr=1.0\n",
    "#         parser.add_argument('--lr2_divide', type=float, default=1.0,\n",
    "#                             help='drop this ratio for the learning rate of the second LSTM')\n",
    "        self.lr2_divide=1.0\n",
    "#         parser.add_argument('--clip', type=float, default=0.25,\n",
    "#                             help='gradient clipping')\n",
    "        self.clip=0.25\n",
    "#         parser.add_argument('--epochs', type=int, default=100,\n",
    "#                             help='upper epoch limit')\n",
    "        self.epochs=100\n",
    "#         parser.add_argument('--batch_size', type=int, default=200, metavar='N',\n",
    "#                             help='batch size')\n",
    "        self.batch_size=200\n",
    "#         parser.add_argument('--small_batch_size', type=int, default=-1,\n",
    "#                             help='the batch size for computation. batch_size should be divisible by small_batch_size.\\\n",
    "#                              In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "#                              until batch_size is reached. An update step is then performed.')\n",
    "        self.small_batch_size=-1\n",
    "#         parser.add_argument('--wdecay', type=float, default=1e-6,\n",
    "#                             help='weight decay applied to all weights')\n",
    "        self.wdecay=1e-6\n",
    "#         parser.add_argument('--nonmono', type=int, default=1,\n",
    "#                             help='decay learning rate after seeing how many validation performance drop')\n",
    "        self.nonmono=1\n",
    "#         parser.add_argument('--training_split_num', type=int, default=2,\n",
    "#                             help='We want to split training corpus into how many subsets. Splitting training dataset seems to make pytorch run much faster and we can store and eval the model more frequently')\n",
    "        self.training_split_num=200\n",
    "#         parser.add_argument('--valid_per_epoch', type=int, default=2,\n",
    "#                             help='Number of times we want to run through validation data and save model within an epoch')\n",
    "        self.valid_per_epoch=200\n",
    "#         parser.add_argument('--copy_training', type=str2bool, nargs='?', default=True,\n",
    "#                             help='turn off this option to save some cpu memory when loading training data')\n",
    "        self.copy_training=True\n",
    "\n",
    "        # system\n",
    "#         parser.add_argument('--seed', type=int, default=1111,\n",
    "#                             help='random seed')\n",
    "        self.seed=1111\n",
    "#         parser.add_argument('--cuda', type=str2bool, nargs='?', default=True,\n",
    "#                             help='use CUDA')\n",
    "        self.cuda=True\n",
    "#         parser.add_argument('--single_gpu', default=False, action='store_true',\n",
    "#                             help='use single GPU')\n",
    "        self.single_gpu=False\n",
    "#         parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "#                             help='report interval')\n",
    "#         self.log-interval=200\n",
    "#         parser.add_argument('--continue_train', action='store_true',\n",
    "#                             help='continue train from a checkpoint')\n",
    "        self.continue_train=False\n",
    "#         parser.add_argument('--start_training_split', type=int, default=0,\n",
    "#                             help='We want to split training corpus into how many subsets. Splitting training dataset seems to make pytorch run much faster and we can store and eval the model more frequently')\n",
    "        self.start_training_split=0\n",
    "\n",
    "args = arguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up environment\n",
      "Experiment dir : /iesl/canvas/hanqingli/NSD_for_sentence_embedding/saves/test1-20200310-171353\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "print(\"Set up environment\")\n",
    "########################\n",
    "assert args.training_split_num >= args.valid_per_epoch\n",
    "\n",
    "if not args.continue_train:\n",
    "    args.save = '{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(args.save, scripts_to_save=['main.ipynb', 'model.py', 'nsd_loss.py'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.coeff_opt == 'maxlc':\n",
    "    current_coeff_opt = 'max'\n",
    "else:\n",
    "    current_coeff_opt = args.coeff_opt\n",
    "\n",
    "#if args.dropoutl < 0:\n",
    "#    args.dropoutl = args.dropouth\n",
    "if args.small_batch_size < 0:\n",
    "    args.small_batch_size = args.batch_size\n",
    "\n",
    "\n",
    "assert args.batch_size % args.small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: <__main__.arguments object at 0x7f2fdfbbdb50>\n"
     ]
    }
   ],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "        sys.stdout.flush()\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')\n",
    "            \n",
    "# Set the random seed manually for reproducibility.\n",
    "seed_all_randomness(args.seed,args.cuda)\n",
    "\n",
    "logging('Args: {}'.format(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "OOV word type percentage: 10.343283933746436%\n",
      "OOV token percentage: 0.748010626181413%\n",
      "loading  /iesl/canvas/hschang/language_modeling/NSD_for_sentence_embedding/resources/glove.840B.300d_filtered_wiki2016.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data\")\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "idx2word_freq, dataloader_train_arr, dataloader_val, dataloader_val_shuffled, max_sent_len = load_corpus(args.data, args.batch_size, args.batch_size, device, args.tensor_folder, args.training_file, args.training_split_num, args.copy_training)\n",
    "\n",
    "\n",
    "def counter_to_tensor(idx2word_freq,device):\n",
    "    total = len(idx2word_freq)\n",
    "    w_freq = torch.zeros(total, dtype=torch.float, device = device, requires_grad = False)\n",
    "    for i in range(total):\n",
    "        w_freq[i] = 1\n",
    "        #w_freq[i] = math.sqrt(idx2word_freq[x][1])\n",
    "        #w_freq[i] = idx2word_freq[x][1]\n",
    "    w_freq[0] = -1\n",
    "    return w_freq\n",
    "\n",
    "external_emb = torch.tensor([0.])\n",
    "extra_init_idx = []\n",
    "if len(args.emb_file) > 0:\n",
    "    #with torch.no_grad():\n",
    "    external_emb, output_emb_size, extra_init_idx = load_emb_file_to_tensor(args.emb_file, device, idx2word_freq)\n",
    "    external_emb = external_emb / (0.000000000001 + external_emb.norm(dim = 1, keepdim=True))\n",
    "    external_emb.requires_grad = args.update_target_emb\n",
    "    print(\"loading \", args.emb_file)\n",
    "else:\n",
    "    if args.target_emb_source == 'ewe':\n",
    "        output_emb_size = args.emsize\n",
    "        print(\"Using word embedding from encoder\")\n",
    "    elif args.target_emb_source == 'rand' and args.update_target_emb == True:\n",
    "        output_emb_size = args.emsize\n",
    "        external_emb = torch.randn(len(idx2word_freq), output_emb_size, device = device, requires_grad = False)\n",
    "        external_emb = external_emb / (0.000000000001 + external_emb.norm(dim = 1, keepdim=True))\n",
    "        external_emb.requires_grad = True\n",
    "        print(\"Initialize target embedding randomly\")\n",
    "    else:\n",
    "        print(\"we don't support such target_emb_source \" + args.target_emb_source + \", update_target_emb \", args.update_target_emb, \", and emb_file \"+ args.emb_file)\n",
    "        sys.exit(1)\n",
    "\n",
    "if args.trans_nhid < 0:\n",
    "    if args.emsize > 0:\n",
    "        args.trans_nhid = args.emsize\n",
    "    else:\n",
    "        args.trans_nhid = output_emb_size\n",
    "\n",
    "\n",
    "w_freq = counter_to_tensor(idx2word_freq,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models\n",
      "Randomly initializes embedding for  30730  words\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "print(\"Building models\")\n",
    "########################\n",
    "\n",
    "ntokens = len(idx2word_freq)\n",
    "\n",
    "encoder = model_code.SEQ2EMB(args.en_model.split('+'), ntokens, args.emsize, args.nhid, args.nlayers,\n",
    "               args.dropout, args.dropouti, args.dropoute, max_sent_len,  external_emb, extra_init_idx, args.encode_trans_layers, args.trans_nhid)\n",
    "criterion = nn.MSELoss().cuda()\n",
    "# if args.nhidlast2 < 0:\n",
    "#     #args.nhidlast2 = output_emb_size\n",
    "#     args.nhidlast2 = encoder.output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as weight_init\n",
    "def initialize_weights(net, normal_std):\n",
    "    for name, param in net.named_parameters(): \n",
    "        if 'bias' in name or 'rnn' not in name:\n",
    "            #print(\"skip \"+name)\n",
    "            continue\n",
    "        print(\"normal init \"+name+\" with std\"+str(normal_std) )\n",
    "        weight_init.normal_(param, std = normal_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.continue_train:\n",
    "    encoder.load_state_dict(torch.load(os.path.join(args.save, 'encoder.pt')))\n",
    "    # decoder.load_state_dict(torch.load(os.path.join(args.save, 'decoder.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder total parameters: 94580450\n"
     ]
    }
   ],
   "source": [
    "parallel_encoder = output_parallel_models(args.cuda, args.single_gpu, encoder)\n",
    "\n",
    "total_params = sum(x.data.nelement() for x in encoder.parameters())\n",
    "logging('Encoder total parameters: {}'.format(total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "print(\"Training\")\n",
    "########################\n",
    "\n",
    "def evaluate(dataloader, external_emb, current_coeff_opt):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    encoder.eval()\n",
    "    # decoder.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_set = 0\n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            feature, target = sample_batched\n",
    "            output_emb_last, _ = parallel_encoder(feature)\n",
    "\n",
    "            compute_target_grad = False\n",
    "            loss = criterion(output_emb_last, target)\n",
    "            total_loss += loss * args.batch_size\n",
    "            # total_loss_set += loss_set * batch_size\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader_train, external_emb, lr, current_coeff_opt, split_i):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.\n",
    "    total_loss_set = 0.\n",
    "    total_loss_set_reg = 0.\n",
    "    total_loss_set_div = 0.\n",
    "    total_loss_set_neg = 0.\n",
    "    total_loss_coeff_pred = 0.\n",
    "\n",
    "    encoder.train()\n",
    "    # decoder.train()\n",
    "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
    "        feature, target = sample_batched\n",
    "        optimizer_e.zero_grad()\n",
    "        output_emb_last, _ = parallel_encoder(feature)\n",
    "        target = target.type(torch.cuda.FloatTensor)\n",
    "        loss = criterion(output_emb_last, target)\n",
    "\n",
    "        loss *= float(args.small_batch_size / args.batch_size)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), args.clip)\n",
    "        # torch.nn.utils.clip_grad_norm_(decoder.parameters(), args.clip)\n",
    "        optimizer_e.step()\n",
    "        if len(args.emb_file) == 0 and args.target_emb_source == 'ewe':\n",
    "            encoder.encoder.weight.data[0, :] = 0\n",
    "\n",
    "        # optimizer_d.step()\n",
    "\n",
    "        if args.update_target_emb:\n",
    "            #print(external_emb.requires_grad)\n",
    "            #print(external_emb.grad)\n",
    "            if args.optimizer == 'SGD':\n",
    "                external_emb.data -= lr/args.lr2_divide/10.0 * external_emb.grad.data\n",
    "            else:\n",
    "                #external_emb.data -= 0.1/args.lr2_divide/10.0 * external_emb.grad.data\n",
    "                external_emb.data -= 10/args.lr2_divide/10.0 * external_emb.grad.data\n",
    "            if args.target_emb_source != 'ewe':\n",
    "                external_emb.data[0,:] = 0\n",
    "            external_emb.grad.data.zero_()\n",
    "            #with torch.no_grad():\n",
    "            external_emb.data = external_emb.data / (0.000000000001 + external_emb.data.norm(dim = 1, keepdim=True))\n",
    "\n",
    "        if i_batch % 200 == 0 and i_batch > 0:\n",
    "            cur_loss = total_loss / 200\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| e {:3d} {:3d} | {:5d}/{:5d} b | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'l {:5.2f}'.format(\n",
    "                epoch, split_i, i_batch, len(dataloader_train.dataset) // args.batch_size, optimizer_e.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / 200, cur_loss))\n",
    "            #if args.coeff_opt == 'maxlc' and current_coeff_opt == 'max' and cur_loss_set + cur_loss_set_neg < -0.02:\n",
    "            if args.coeff_opt == 'maxlc' and current_coeff_opt == 'max':\n",
    "                current_coeff_opt = 'lc'\n",
    "                print(\"switch to lc\")\n",
    "            total_loss = 0.\n",
    "            start_time = time.time()\n",
    "            break\n",
    "    return current_coeff_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.optimizer == 'SGD':\n",
    "    optimizer_e = torch.optim.SGD(encoder.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "    # optimizer_d = torch.optim.SGD(decoder.parameters(), lr=args.lr/args.lr2_divide, weight_decay=args.wdecay)\n",
    "else:\n",
    "    optimizer_e = torch.optim.Adam(encoder.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "    # optimizer_d = torch.optim.Adam(decoder.parameters(), lr=args.lr/args.lr2_divide, weight_decay=args.wdecay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.continue_train:\n",
    "    optimizer_e_state_dict = torch.load(os.path.join(args.save, 'optimizer_e.pt'), map_location=device)\n",
    "    optimizer_e.load_state_dict(optimizer_e_state_dict)\n",
    "    # optimizer_d_state_dict = torch.load(os.path.join(args.save, 'optimizer_d.pt'), map_location=device)\n",
    "    # optimizer_d.load_state_dict(optimizer_d_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanqingli/workspace/NSD_for_sentence_embedding/src/utils.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature = torch.tensor(self.feature[idx, :], dtype = torch.long, device = self.output_device)\n",
      "/home/hanqingli/workspace/NSD_for_sentence_embedding/src/utils.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(self.target[idx, :], dtype = torch.long, device = self.output_device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| e   1   0 |   200/  217 b | lr 1.00 | ms/batch 372.88 | l  0.44\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss_alll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4470ea12f71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mval_loss_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_coeff_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss_alll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         logging('| end of epoch {:3d} split {:3d} | time: {:5.2f}s | lr {:5.2f} | valid loss {:5.2f}'\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss_alll' is not defined"
     ]
    }
   ],
   "source": [
    "lr = args.lr\n",
    "best_val_loss = None\n",
    "nonmono_count = 0\n",
    "saving_freq = int(math.floor(args.training_split_num / args.valid_per_epoch))\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    for i in range(len(dataloader_train_arr)):\n",
    "        if epoch == 1 and i < args.start_training_split:\n",
    "            print(\"Skipping epoch \"+str(epoch) + ' split '+str(i) )\n",
    "            continue\n",
    "        current_coeff_opt = train_one_epoch(dataloader_train_arr[i], external_emb, lr, current_coeff_opt, i)\n",
    "        \n",
    "        if i != args.training_split_num - 1 and (i + 1) % saving_freq != 0:\n",
    "            continue\n",
    "\n",
    "        val_loss_all = evaluate(dataloader_val, external_emb, current_coeff_opt)\n",
    "        print(val_loss_all)\n",
    "        logging('-' * 89)\n",
    "        logging('| end of epoch {:3d} split {:3d} | time: {:5.2f}s | lr {:5.2f} | valid loss {:5.2f}'\n",
    "                .format(epoch, i, (time.time() - epoch_start_time), lr,\n",
    "                                           val_loss_all))\n",
    "        \n",
    "        val_loss_important = val_loss_all\n",
    "        \n",
    "        logging('-' * 89)\n",
    "        #dataloader_val_shuffled?\n",
    "        val_loss_all = evaluate(dataloader_val_shuffled, external_emb, current_coeff_opt)\n",
    "        logging('-' * 89)\n",
    "        logging('| Shuffled | time: {:5.2f}s | lr {:5.2f} | valid loss {:5.2f}'\n",
    "                .format((time.time() - epoch_start_time), lr,\n",
    "                                           val_loss_all))\n",
    "        logging('-' * 89)\n",
    "        \n",
    "        if not best_val_loss or val_loss_important < best_val_loss:\n",
    "            # save_checkpoint(encoder, decoder, optimizer_e, optimizer_d, external_emb, args.save)\n",
    "            save_checkpoint(encoder, optimizer_e, external_emb, args.save)\n",
    "            best_val_loss = val_loss_important\n",
    "            logging('Models Saved')\n",
    "        else:\n",
    "            nonmono_count += 1\n",
    "        \n",
    "        if nonmono_count >= args.nonmono:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            nonmono_count = 0\n",
    "            lr /= 4.0\n",
    "            for param_group in optimizer_e.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            # for param_group in optimizer_d.param_groups:\n",
    "            #     param_group['lr'] = lr/args.lr2_divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            if 'cuda' in str(obj.device):\n",
    "                print(obj.device)\n",
    "                del obj\n",
    "                torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "853px",
    "left": "1550px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
